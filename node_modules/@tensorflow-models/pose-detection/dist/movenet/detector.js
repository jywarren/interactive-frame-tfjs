"use strict";
/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __generator = (this && this.__generator) || function (thisArg, body) {
    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;
    return g = { next: verb(0), "throw": verb(1), "return": verb(2) }, typeof Symbol === "function" && (g[Symbol.iterator] = function() { return this; }), g;
    function verb(n) { return function (v) { return step([n, v]); }; }
    function step(op) {
        if (f) throw new TypeError("Generator is already executing.");
        while (_) try {
            if (f = 1, y && (t = op[0] & 2 ? y["return"] : op[0] ? y["throw"] || ((t = y["return"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;
            if (y = 0, t) op = [op[0] & 2, t.value];
            switch (op[0]) {
                case 0: case 1: t = op; break;
                case 4: _.label++; return { value: op[1], done: false };
                case 5: _.label++; y = op[1]; op = [0]; continue;
                case 7: op = _.ops.pop(); _.trys.pop(); continue;
                default:
                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }
                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }
                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }
                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }
                    if (t[2]) _.ops.pop();
                    _.trys.pop(); continue;
            }
            op = body.call(thisArg, _);
        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }
        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };
    }
};
Object.defineProperty(exports, "__esModule", { value: true });
var tfc = require("@tensorflow/tfjs-converter");
var tf = require("@tensorflow/tfjs-core");
var constants_1 = require("../calculators/constants");
var image_utils_1 = require("../calculators/image_utils");
var is_video_1 = require("../calculators/is_video");
var keypoints_one_euro_filter_1 = require("../calculators/keypoints_one_euro_filter");
var low_pass_filter_1 = require("../calculators/low_pass_filter");
var constants_2 = require("../constants");
var types_1 = require("../types");
var util_1 = require("../util");
var constants_3 = require("./constants");
var detector_utils_1 = require("./detector_utils");
/**
 * MoveNet detector class.
 */
var MoveNetDetector = /** @class */ (function () {
    function MoveNetDetector(moveNetModel, config) {
        this.moveNetModel = moveNetModel;
        this.modelInputResolution = { height: 0, width: 0 };
        this.keypointIndexByName = util_1.getKeypointIndexByName(types_1.SupportedModels.MoveNet);
        // Global states.
        this.keypointsFilter = new keypoints_one_euro_filter_1.KeypointsOneEuroFilter(constants_3.KEYPOINT_FILTER_CONFIG);
        this.cropRegionFilterYMin = new low_pass_filter_1.LowPassFilter(constants_3.CROP_FILTER_ALPHA);
        this.cropRegionFilterXMin = new low_pass_filter_1.LowPassFilter(constants_3.CROP_FILTER_ALPHA);
        this.cropRegionFilterYMax = new low_pass_filter_1.LowPassFilter(constants_3.CROP_FILTER_ALPHA);
        this.cropRegionFilterXMax = new low_pass_filter_1.LowPassFilter(constants_3.CROP_FILTER_ALPHA);
        if (config.modelType === constants_3.SINGLEPOSE_LIGHTNING) {
            this.modelInputResolution.width = constants_3.MOVENET_SINGLEPOSE_LIGHTNING_RESOLUTION;
            this.modelInputResolution.height =
                constants_3.MOVENET_SINGLEPOSE_LIGHTNING_RESOLUTION;
        }
        else if (config.modelType === constants_3.SINGLEPOSE_THUNDER) {
            this.modelInputResolution.width = constants_3.MOVENET_SINGLEPOSE_THUNDER_RESOLUTION;
            this.modelInputResolution.height = constants_3.MOVENET_SINGLEPOSE_THUNDER_RESOLUTION;
        }
        this.enableSmoothing = config.enableSmoothing;
    }
    /**
     * Runs inference on an image using a model that is assumed to be a person
     * keypoint model that outputs 17 keypoints.
     * @param inputImage 4D tensor containing the input image. Should be of size
     *     [1, modelHeight, modelWidth, 3].
     * @param executeSync Whether to execute the model synchronously.
     * @return An InferenceResult with keypoints and scores, or null if the
     *     inference call could not be executed (for example when the model was
     *     not initialized yet) or if it produced an unexpected tensor size.
     */
    MoveNetDetector.prototype.detectKeypoints = function (inputImage, executeSync) {
        if (executeSync === void 0) { executeSync = true; }
        return __awaiter(this, void 0, void 0, function () {
            var numKeypoints, outputTensor, inferenceResult, keypoints, i;
            return __generator(this, function (_a) {
                switch (_a.label) {
                    case 0:
                        if (!this.moveNetModel) {
                            return [2 /*return*/, null];
                        }
                        numKeypoints = 17;
                        if (!executeSync) return [3 /*break*/, 1];
                        outputTensor = this.moveNetModel.execute(inputImage);
                        return [3 /*break*/, 3];
                    case 1: return [4 /*yield*/, this.moveNetModel.executeAsync(inputImage)];
                    case 2:
                        outputTensor =
                            (_a.sent());
                        _a.label = 3;
                    case 3:
                        // We expect an output array of shape [1, 1, 17, 3] (batch, person,
                        // keypoint, (y, x, score)).
                        if (!outputTensor || outputTensor.shape.length !== 4 ||
                            outputTensor.shape[0] !== 1 || outputTensor.shape[1] !== 1 ||
                            outputTensor.shape[2] !== numKeypoints || outputTensor.shape[3] !== 3) {
                            outputTensor.dispose();
                            return [2 /*return*/, null];
                        }
                        if (!(tf.getBackend() !== 'webgpu')) return [3 /*break*/, 4];
                        inferenceResult = outputTensor.dataSync();
                        return [3 /*break*/, 6];
                    case 4: return [4 /*yield*/, outputTensor.data()];
                    case 5:
                        inferenceResult = _a.sent();
                        _a.label = 6;
                    case 6:
                        outputTensor.dispose();
                        keypoints = [];
                        for (i = 0; i < numKeypoints; ++i) {
                            keypoints[i] = {
                                y: inferenceResult[i * 3],
                                x: inferenceResult[i * 3 + 1],
                                score: inferenceResult[i * 3 + 2]
                            };
                        }
                        return [2 /*return*/, keypoints];
                }
            });
        });
    };
    /**
     * Estimates poses for an image or video frame.
     *
     * This does standard ImageNet pre-processing before inferring through the
     * model. The image should pixels should have values [0-255]. It returns a
     * single pose.
     *
     * @param image ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement
     * The input image to feed through the network.
     *
     * @param config Optional. A configuration object with the following
     * properties:
     *  `maxPoses`: Optional. Has to be set to 1.
     *
     * @param timestamp Optional. In milliseconds. This is useful when image is
     *     a tensor, which doesn't have timestamp info. Or to override timestamp
     *     in a video.
     *
     * @return An array of `Pose`s.
     */
    MoveNetDetector.prototype.estimatePoses = function (image, estimationConfig, timestamp) {
        if (estimationConfig === void 0) { estimationConfig = constants_3.MOVENET_SINGLE_POSE_ESTIMATION_CONFIG; }
        return __awaiter(this, void 0, void 0, function () {
            var imageTensor3D, imageSize, imageTensor4D, croppedImage, keypoints, i, newCropRegion, numValidKeypoints, poseScore, i, pose;
            var _this = this;
            return __generator(this, function (_a) {
                switch (_a.label) {
                    case 0:
                        estimationConfig = detector_utils_1.validateEstimationConfig(estimationConfig);
                        if (image == null) {
                            this.reset();
                            return [2 /*return*/, []];
                        }
                        if (timestamp == null) {
                            if (is_video_1.isVideo(image)) {
                                timestamp = image.currentTime * constants_1.SECOND_TO_MICRO_SECONDS;
                            }
                        }
                        else {
                            timestamp = timestamp * constants_1.MILLISECOND_TO_MICRO_SECONDS;
                        }
                        imageTensor3D = image_utils_1.toImageTensor(image);
                        imageSize = image_utils_1.getImageSize(imageTensor3D);
                        imageTensor4D = tf.expandDims(imageTensor3D, 0);
                        // Make sure we don't dispose the input image if it's already a tensor.
                        if (!(image instanceof tf.Tensor)) {
                            imageTensor3D.dispose();
                        }
                        if (!this.cropRegion) {
                            this.cropRegion = this.initCropRegion(imageSize.width, imageSize.height);
                        }
                        croppedImage = tf.tidy(function () {
                            // Crop region is a [batch, 4] size tensor.
                            var cropRegionTensor = tf.tensor2d([[
                                    _this.cropRegion.yMin, _this.cropRegion.xMin, _this.cropRegion.yMax,
                                    _this.cropRegion.xMax
                                ]]);
                            // The batch index that the crop should operate on. A [batch] size
                            // tensor.
                            var boxInd = tf.zeros([1], 'int32');
                            // Target size of each crop.
                            var cropSize = [_this.modelInputResolution.height, _this.modelInputResolution.width];
                            return tf.cast(tf.image.cropAndResize(imageTensor4D, cropRegionTensor, boxInd, cropSize, 'bilinear', 0), 'int32');
                        });
                        imageTensor4D.dispose();
                        return [4 /*yield*/, this.detectKeypoints(croppedImage)];
                    case 1:
                        keypoints = _a.sent();
                        croppedImage.dispose();
                        if (keypoints == null) {
                            this.reset();
                            return [2 /*return*/, []];
                        }
                        // Convert keypoints from crop coordinates to image coordinates.
                        for (i = 0; i < keypoints.length; ++i) {
                            keypoints[i].y =
                                this.cropRegion.yMin + keypoints[i].y * this.cropRegion.height;
                            keypoints[i].x =
                                this.cropRegion.xMin + keypoints[i].x * this.cropRegion.width;
                        }
                        // Apply the sequential filter before estimating the cropping area to make
                        // it more stable.
                        if (timestamp != null && this.enableSmoothing) {
                            keypoints =
                                this.keypointsFilter.apply(keypoints, timestamp, 1 /* objectScale */);
                        }
                        newCropRegion = this.determineCropRegion(keypoints, imageSize.height, imageSize.width);
                        this.cropRegion = this.filterCropRegion(newCropRegion);
                        numValidKeypoints = 0.0;
                        poseScore = 0.0;
                        for (i = 0; i < keypoints.length; ++i) {
                            keypoints[i].name = constants_2.COCO_KEYPOINTS[i];
                            keypoints[i].y *= imageSize.height;
                            keypoints[i].x *= imageSize.width;
                            if (keypoints[i].score > constants_3.MIN_CROP_KEYPOINT_SCORE) {
                                ++numValidKeypoints;
                                poseScore += keypoints[i].score;
                            }
                        }
                        if (numValidKeypoints > 0) {
                            poseScore /= numValidKeypoints;
                        }
                        else {
                            // No pose detected, so reset all filters.
                            this.resetFilters();
                        }
                        pose = { score: poseScore, keypoints: keypoints };
                        return [2 /*return*/, [pose]];
                }
            });
        });
    };
    MoveNetDetector.prototype.filterCropRegion = function (newCropRegion) {
        if (!newCropRegion) {
            this.cropRegionFilterYMin.reset();
            this.cropRegionFilterXMin.reset();
            this.cropRegionFilterYMax.reset();
            this.cropRegionFilterXMax.reset();
            return null;
        }
        else {
            var filteredYMin = this.cropRegionFilterYMin.apply(newCropRegion.yMin);
            var filteredXMin = this.cropRegionFilterXMin.apply(newCropRegion.xMin);
            var filteredYMax = this.cropRegionFilterYMax.apply(newCropRegion.yMax);
            var filteredXMax = this.cropRegionFilterXMax.apply(newCropRegion.xMax);
            return {
                yMin: filteredYMin,
                xMin: filteredXMin,
                yMax: filteredYMax,
                xMax: filteredXMax,
                height: filteredYMax - filteredYMin,
                width: filteredXMax - filteredXMin
            };
        }
    };
    MoveNetDetector.prototype.dispose = function () {
        this.moveNetModel.dispose();
    };
    MoveNetDetector.prototype.reset = function () {
        this.cropRegion = null;
        this.resetFilters();
    };
    MoveNetDetector.prototype.resetFilters = function () {
        this.keypointsFilter.reset();
        this.cropRegionFilterYMin.reset();
        this.cropRegionFilterXMin.reset();
        this.cropRegionFilterYMax.reset();
        this.cropRegionFilterXMax.reset();
    };
    MoveNetDetector.prototype.torsoVisible = function (keypoints) {
        return ((keypoints[this.keypointIndexByName['left_hip']].score >
            constants_3.MIN_CROP_KEYPOINT_SCORE ||
            keypoints[this.keypointIndexByName['right_hip']].score >
                constants_3.MIN_CROP_KEYPOINT_SCORE) &&
            (keypoints[this.keypointIndexByName['left_shoulder']].score >
                constants_3.MIN_CROP_KEYPOINT_SCORE ||
                keypoints[this.keypointIndexByName['right_shoulder']].score >
                    constants_3.MIN_CROP_KEYPOINT_SCORE));
    };
    /**
     * Calculates the maximum distance from each keypoints to the center location.
     * The function returns the maximum distances from the two sets of keypoints:
     * full 17 keypoints and 4 torso keypoints. The returned information will be
     * used to determine the crop size. See determineCropRegion for more detail.
     *
     * @param targetKeypoints Maps from joint names to coordinates.
     */
    MoveNetDetector.prototype.determineTorsoAndBodyRange = function (keypoints, targetKeypoints, centerY, centerX) {
        var torsoJoints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip'];
        var maxTorsoYrange = 0.0;
        var maxTorsoXrange = 0.0;
        for (var i = 0; i < torsoJoints.length; i++) {
            var distY = Math.abs(centerY - targetKeypoints[torsoJoints[i]][0]);
            var distX = Math.abs(centerX - targetKeypoints[torsoJoints[i]][1]);
            if (distY > maxTorsoYrange) {
                maxTorsoYrange = distY;
            }
            if (distX > maxTorsoXrange) {
                maxTorsoXrange = distX;
            }
        }
        var maxBodyYrange = 0.0;
        var maxBodyXrange = 0.0;
        for (var _i = 0, _a = Object.keys(targetKeypoints); _i < _a.length; _i++) {
            var key = _a[_i];
            if (keypoints[this.keypointIndexByName[key]].score <
                constants_3.MIN_CROP_KEYPOINT_SCORE) {
                continue;
            }
            var distY = Math.abs(centerY - targetKeypoints[key][0]);
            var distX = Math.abs(centerX - targetKeypoints[key][1]);
            if (distY > maxBodyYrange) {
                maxBodyYrange = distY;
            }
            if (distX > maxBodyXrange) {
                maxBodyXrange = distX;
            }
        }
        return [maxTorsoYrange, maxTorsoXrange, maxBodyYrange, maxBodyXrange];
    };
    /**
     * Determines the region to crop the image for the model to run inference on.
     * The algorithm uses the detected joints from the previous frame to estimate
     * the square region that encloses the full body of the target person and
     * centers at the midpoint of two hip joints. The crop size is determined by
     * the distances between each joints and the center point.
     * When the model is not confident with the four torso joint predictions, the
     * function returns a default crop which is the full image padded to square.
     */
    MoveNetDetector.prototype.determineCropRegion = function (keypoints, imageHeight, imageWidth) {
        var targetKeypoints = {};
        for (var _i = 0, COCO_KEYPOINTS_1 = constants_2.COCO_KEYPOINTS; _i < COCO_KEYPOINTS_1.length; _i++) {
            var key = COCO_KEYPOINTS_1[_i];
            targetKeypoints[key] = [
                keypoints[this.keypointIndexByName[key]].y * imageHeight,
                keypoints[this.keypointIndexByName[key]].x * imageWidth
            ];
        }
        if (this.torsoVisible(keypoints)) {
            var centerY = (targetKeypoints['left_hip'][0] + targetKeypoints['right_hip'][0]) /
                2;
            var centerX = (targetKeypoints['left_hip'][1] + targetKeypoints['right_hip'][1]) /
                2;
            var _a = this.determineTorsoAndBodyRange(keypoints, targetKeypoints, centerY, centerX), maxTorsoYrange = _a[0], maxTorsoXrange = _a[1], maxBodyYrange = _a[2], maxBodyXrange = _a[3];
            var cropLengthHalf = Math.max(maxTorsoXrange * 1.9, maxTorsoYrange * 1.9, maxBodyYrange * 1.2, maxBodyXrange * 1.2);
            cropLengthHalf = Math.min(cropLengthHalf, Math.max(centerX, imageWidth - centerX, centerY, imageHeight - centerY));
            var cropCorner = [centerY - cropLengthHalf, centerX - cropLengthHalf];
            if (cropLengthHalf > Math.max(imageWidth, imageHeight) / 2) {
                return this.initCropRegion(imageHeight, imageWidth);
            }
            else {
                var cropLength = cropLengthHalf * 2;
                return {
                    yMin: cropCorner[0] / imageHeight,
                    xMin: cropCorner[1] / imageWidth,
                    yMax: (cropCorner[0] + cropLength) / imageHeight,
                    xMax: (cropCorner[1] + cropLength) / imageWidth,
                    height: (cropCorner[0] + cropLength) / imageHeight -
                        cropCorner[0] / imageHeight,
                    width: (cropCorner[1] + cropLength) / imageWidth -
                        cropCorner[1] / imageWidth
                };
            }
        }
        else {
            return this.initCropRegion(imageHeight, imageWidth);
        }
    };
    /**
     * Provides initial crop region.
     *
     * The function provides the initial crop region when the algorithm cannot
     * reliably determine the crop region from the previous frame. There are two
     * scenarios:
     *   1) The very first frame: the function returns the best quess by cropping
     *      a square in the middle of the image.
     *   2) Not enough reliable keypoints detected from the previous frame: the
     *      function pads the full image from both sides to make it a square
     *      image.
     */
    MoveNetDetector.prototype.initCropRegion = function (imageHeight, imageWidth) {
        var boxHeight, boxWidth, yMin, xMin;
        if (!this.cropRegion) {
            // If it is the first frame, perform a best guess by making the square
            // crop at the image center to better utilize the image pixels and
            // create higher chance to enter the cropping loop.
            if (imageWidth > imageHeight) {
                boxHeight = 1.0;
                boxWidth = imageHeight / imageWidth;
                yMin = 0.0;
                xMin = (imageWidth / 2 - imageHeight / 2) / imageWidth;
            }
            else {
                boxHeight = imageWidth / imageHeight;
                boxWidth = 1.0;
                yMin = (imageHeight / 2 - imageWidth / 2) / imageHeight;
                xMin = 0.0;
            }
        }
        else {
            // No cropRegion was available from a previous estimatePoses() call, so
            // run the model on the full image with padding on both sides.
            if (imageWidth > imageHeight) {
                boxHeight = imageWidth / imageHeight;
                boxWidth = 1.0;
                yMin = (imageHeight / 2 - imageWidth / 2) / imageHeight;
                xMin = 0.0;
            }
            else {
                boxHeight = 1.0;
                boxWidth = imageHeight / imageWidth;
                yMin = 0.0;
                xMin = (imageWidth / 2 - imageHeight / 2) / imageWidth;
            }
        }
        return {
            yMin: yMin,
            xMin: xMin,
            yMax: yMin + boxHeight,
            xMax: xMin + boxWidth,
            height: boxHeight,
            width: boxWidth
        };
    };
    return MoveNetDetector;
}());
/**
 * Loads the MoveNet model instance from a checkpoint. The model to be loaded
 * is configurable using the config dictionary `ModelConfig`. Please find more
 * details in the documentation of the `ModelConfig`.
 *
 * @param config `ModelConfig` dictionary that contains parameters for
 * the MoveNet loading process. Please find more details of each parameter
 * in the documentation of the `ModelConfig` interface.
 */
function load(modelConfig) {
    if (modelConfig === void 0) { modelConfig = constants_3.MOVENET_CONFIG; }
    return __awaiter(this, void 0, void 0, function () {
        var config, model, modelUrl;
        return __generator(this, function (_a) {
            switch (_a.label) {
                case 0:
                    config = detector_utils_1.validateModelConfig(modelConfig);
                    if (!config.modelUrl) return [3 /*break*/, 2];
                    return [4 /*yield*/, tfc.loadGraphModel(config.modelUrl)];
                case 1:
                    model = _a.sent();
                    return [3 /*break*/, 4];
                case 2:
                    modelUrl = void 0;
                    if (config.modelType === constants_3.SINGLEPOSE_LIGHTNING) {
                        modelUrl = constants_3.MOVENET_SINGLEPOSE_LIGHTNING_URL;
                    }
                    else if (config.modelType === constants_3.SINGLEPOSE_THUNDER) {
                        modelUrl = constants_3.MOVENET_SINGLEPOSE_THUNDER_URL;
                    }
                    return [4 /*yield*/, tfc.loadGraphModel(modelUrl, { fromTFHub: true })];
                case 3:
                    model = _a.sent();
                    _a.label = 4;
                case 4: return [2 /*return*/, new MoveNetDetector(model, config)];
            }
        });
    });
}
exports.load = load;
//# sourceMappingURL=detector.js.map